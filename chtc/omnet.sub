# 
# HTCondor Submit file 
#
# Objective: Train feature pyramid network using HTCondor GPU machines

# Must set the universe to Docker
universe = docker
docker_image = bliptrip/detectron2:torch-1.10.0-cuda11.3-cudnn8-runtime

# set the log, error and output files 
log = d2.log.txt
error = d2.err.txt
output = d2.out.txt

# set the executable to run
executable = d2.sh
arguments = --annotations images/d2annotations.json -b 8 -e 4096 --batches-per-image 1024 --model-config configs/mask_rcnn_X_101_32x8d_FPN_3x.yaml --checkpoint models/model_final_2d9806.pkl -s 390263 --mask-map maps/d2.1.json  

# take our python script to the compute node
transfer_input_files = d2_train.py,d2.tar.xz,models.tar.xz,images.tar.xz
transfer_output_files = models.tar.xz

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# We require a machine that can support the version of the CUDA driver used in the Docker image
# The Ampere generation GPUS (e.g. A100) cannot run with CUDA 10.1 so add a
# CUDACapability requirement to avoid running there
#Requirements = (Target.CUDADriverVersion >= 11.3) && (CUDACapability >= 7.5) && (CUDAGlobalMemoryMb >= 16384)

# We must request 1 CPU in addition to 1 GPU
request_cpus = 1
request_gpus = 1

# select some memory and disk space
request_memory = 16GB
request_disk = 100GB

# Opt in to using CHTC GPU Lab resources
+WantGPULab = true
# Specify short job type to run more GPUs in parallel
# Can request "short", "medium" or "long"
+GPUJobLength = "short"

# Tell HTCondor to run 1 instances of our job:
queue 1
